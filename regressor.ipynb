{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Dependencies\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import matthews_corrcoef, mean_squared_error, accuracy_score, make_scorer\n",
    "\n",
    "# Model Persistence\n",
    "from joblib import dump, load\n",
    "\n",
    "# Plotter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Argument Parser\n",
    "import argparse\n",
    "\n",
    "# Write to a log file\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the logger\n",
    "def log_files():\n",
    "    \"\"\"\n",
    "    Create the meachanism for which we log results to a .log file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    logger:  The logger object we create to call on in other functions. \n",
    "    \"\"\"\n",
    "\n",
    "    # Instantiate the logger and set the formatting and minimum level to DEBUG.\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "    # Display the logs in the output\n",
    "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "    stdout_handler.setLevel(logging.DEBUG)\n",
    "    stdout_handler.setFormatter(formatter)\n",
    "\n",
    "    # Write the logs to a file\n",
    "    file_handler = logging.FileHandler('threshold.log')\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Adding the file and output handlers to the logger.\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(stdout_handler)\n",
    "    return logger\n",
    "\n",
    "logger = log_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the complete dataset.\n",
    "def import_data():\n",
    "    \"\"\"\n",
    "    Import the full dataset from the current path.  Also apply some of the necessary preprocessing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df:  Dataframe of the full KI training dataset, with any values above 50,000 removed.\n",
    "\n",
    "    base_range:  Contains the range of values within the dataframe for rescaling purposes.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Importing the full KI set into a dataframe.\n",
    "    path = os.getcwd()\n",
    "    df = pd.read_csv(path + '/PositivePeptide_Ki.csv')\n",
    "    logger.debug('The full dataset has %i examples.' %(len(df)))\n",
    "\n",
    "    # Rescaling the dataframe in the log10 (-5,5) range.\n",
    "    df['KI (nM) rescaled'], base_range  = rescale(df['KI (nM)'], destination_interval=(-5,5))\n",
    "\n",
    "    return df, base_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logarithmically scalling the values.\n",
    "def rescale(array=np.array(0), destination_interval=(-5,5)):\n",
    "    \"\"\"\n",
    "    Rescale the KI values from nM to a log scale within the range of\n",
    "        a given destination interval.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array:  A numpy array of KI values, in nM.\n",
    "\n",
    "    destination_interval: the interval that we set the range of the log scale to\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array:  Transformed array into the log scale.\n",
    "    \n",
    "    saved_range:  The (min, max) range of the original given array.  Used if we need\n",
    "        to rescale back into \"KI (nM)\" form.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Rescaling the values and saving the initial range.\n",
    "    array = np.log(array)\n",
    "    saved_range = (array.min(), array.max())\n",
    "    array = np.interp(array, saved_range, destination_interval)\n",
    "\n",
    "    return array, saved_range\n",
    "\n",
    "## Inverse of the rescale function to rescale the outputs.\n",
    "def unscale(array, destination_interval, source_interval=(-5,5)):\n",
    "    \"\"\"\n",
    "    Rescales an array of log-transformed values back into \"KI (nM)\" form.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array:  A numpy array of KI values in log-transformed form.\n",
    "\n",
    "    destination_interval:  The original range of KI values.\n",
    "\n",
    "    source_interval: The current range of KI log transformed values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array:  A numpy array of the KI values back in the original format.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Undoing the previous rescaling.\n",
    "    array = np.interp(array, source_interval, destination_interval)\n",
    "    array = np.exp(array)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward selection for our classifier.\n",
    "def fs_classifier(x, y, model):\n",
    "    \"\"\"\n",
    "    Perform Sequential Forward Selection on the given dataset, but for \n",
    "        the classifer portion of the model.  MCC is the scorer used.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: Input values of the dataset.\n",
    "\n",
    "    y: Output values for the different classes of the dataset.\n",
    "\n",
    "    model: Model function used for Sequential Feature Selection.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: Input values of the dataset with half of the features selected.\n",
    "\n",
    "    sfs:  The SequentialFeatureSelector model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit a feature selector to SVM w/RBF kernel classifier and use the 'accuracy' score.\n",
    "    logger.debug('Forward Selection Starting')\n",
    "    sfs = SequentialFeatureSelector(model, n_jobs=-1, scoring=make_scorer(matthews_corrcoef))\n",
    "    sfs.fit(x, y)\n",
    "    x = sfs.transform(x)\n",
    "    logger.debug('Forward Selection Finished')\n",
    "\n",
    "    return x, sfs\n",
    "\n",
    "\n",
    "## Code for Principal Component Analysis\n",
    "def principal_component_analysis(x):\n",
    "    \"\"\"\n",
    "    Perform PCA and return the transformed inputs with the principal components.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: Input values to perform PCA on.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: x input transformed with PCA.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Run PCA on the given inputs.\n",
    "    logger.debug('PCA Starting')\n",
    "    pca = PCA()\n",
    "    pca.fit(x)\n",
    "    x = pca.transform(x)\n",
    "    logger.debug('PCA Finished')\n",
    "\n",
    "    return x, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimizer(x, y, params, model=SVC()):\n",
    "    \"\"\"\n",
    "    Perform GridSearchCV to find and return the best hyperparmeters.  I'll use MCC score here.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: Input values to perform GridSearchCV with.\n",
    "\n",
    "    y: Output values to create GridSearchCV with.\n",
    "\n",
    "    params: Dictionary of parameters to run GridSearchCV on.\n",
    "\n",
    "    model: The model that we are using for GridSearchCV\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bestvals: Optimzied hyperparameters for the model that we are running the search on.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Use GridsearchCV to get the optimized parameters.\n",
    "    logger.debug('GridSearchCV Starting')\n",
    "    clf = GridSearchCV(model, params, scoring=make_scorer(matthews_corrcoef), cv=5, n_jobs=-1)\n",
    "    clf.fit(x,y)\n",
    "\n",
    "    # Showing the best paramets found on the development set.\n",
    "    logger.debug('Best parameters set found on development set:')\n",
    "    logger.debug('')\n",
    "    logger.debug(clf.best_params_)\n",
    "    logger.debug('')\n",
    "\n",
    "    # Testing on the development set.\n",
    "    logger.debug('Grid scores on development set:')\n",
    "    logger.debug('')\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        logger.debug('%0.3f (+/-%0.03f) for %r' % (mean, std*2, params))\n",
    "    logger.debug('')\n",
    "\n",
    "    # Save the best parameters.\n",
    "    bestvals = clf.best_params_\n",
    "\n",
    "    return bestvals\n",
    "\n",
    "\n",
    "def classifier_trainer(x, y, params, model=SVC()):\n",
    "    \"\"\"\n",
    "    Perform fitting on the reduced datasets and then make predictions.  The output values are in the log file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: Reduced set of input values.\n",
    "\n",
    "    y: Output KI values that we are using for the training and validation sets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    # Train our model\n",
    "    seeds = [33, 42, 55, 68, 74]\n",
    "    i = 0\n",
    "\n",
    "    # Initialize the sums of the acc/mcc's.\n",
    "    train_accuracy_sum = 0\n",
    "    train_mcc_sum = 0\n",
    "    valid_accuracy_sum = 0\n",
    "    valid_mcc_sum = 0\n",
    "\n",
    "    optimized_features = hyperparameter_optimizer(x, y, params, model)\n",
    "\n",
    "    model.set_params(**optimized_features)\n",
    "\n",
    "    for seed in seeds:\n",
    "        i += 1\n",
    "        logger.debug('Training:')\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=seed)\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        logger.debug('Training Finished.')\n",
    "\n",
    "        # Test the model on the training set.\n",
    "        y_train_pred = model.predict(x_train)\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        train_mcc = matthews_corrcoef(y_train, y_train_pred)\n",
    "\n",
    "        # Test the model on the validation set.\n",
    "        y_valid_pred = model.predict(x_valid)\n",
    "        valid_accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "        valid_mcc = matthews_corrcoef(y_valid, y_valid_pred)\n",
    "\n",
    "        # Log the individual folds\n",
    "        logger.info('Training Accuracy: %3.3f, Training MCC: %3.3f, Validation Accuracy: %3.3f, '\n",
    "                    'Validation MCC: %3.3f, Fold: %i'\n",
    "                    %(train_accuracy, train_mcc, valid_accuracy, valid_mcc, i))\n",
    "\n",
    "        # Add to the sums\n",
    "        train_accuracy_sum += train_accuracy\n",
    "        train_mcc_sum += train_mcc\n",
    "        valid_accuracy_sum += valid_accuracy\n",
    "        valid_mcc_sum += valid_mcc\n",
    "    \n",
    "    # Calculate the averages\n",
    "    train_accuracy_avg = train_accuracy_sum/5\n",
    "    train_mcc_avg = train_mcc_sum/5\n",
    "    valid_accuracy_avg = valid_accuracy_sum/5\n",
    "    valid_mcc_avg = valid_mcc_sum/5\n",
    "\n",
    "    # Log the average scores for all the folds\n",
    "    logger.info('AVG Training Accuracy: %3.3f, AVG Training MCC: %3.3f, AVG Validation Accuracy: %3.3f, '\n",
    "                'AVG Validation MCC: %3.3f' %(train_accuracy_avg, train_mcc_avg, valid_accuracy_avg, valid_mcc_avg))\n",
    "\n",
    "def classifier_pipeline(x, y, model, params):\n",
    "    \"\"\"\n",
    "    This function is our pipeline for the bucket classifier.  The outputs are recorded into a log file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: Input variables\n",
    "    \n",
    "    y: Output classes\n",
    "    \n",
    "    model: Model that we are using\n",
    "    \n",
    "    \"\"\"\n",
    "    x, _ = fs_classifier(x, y, model)\n",
    "    x, _ = principal_component_analysis(x)\n",
    "    classifier_trainer(x, y, params, model)\n",
    "\n",
    "\n",
    "# Function to separate items into buckets.\n",
    "def bucket_seperator(threshold):\n",
    "    \"\"\"\n",
    "    This function uses a classification threshold to split the data into large and small buckets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    threshold: Threshold value to set the KI classification to.\n",
    "\n",
    "    df: Pandas DataFrame that is being \"classified\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bucket: Dummy variable for now, returns a dummy variable\n",
    "    \"\"\"\n",
    "    \n",
    "    # Import the data.\n",
    "    df, _ = import_data()\n",
    "\n",
    "    # Creates a column in our dataframe to classify into 3 separate buckets.  A 'small' and 'large' bucket\n",
    "    # based on the threshold, and a 'do not measure bucket' for anything with a KI value of > 4000\n",
    "    df['Bucket'] = pd.cut(x=df['KI (nM)'], bins=(0, threshold, 4000, float('inf')), labels=(1,2,3))\n",
    "\n",
    "    # Try basing the threshold off of log transform?\n",
    "\n",
    "    large_bucket_count = df[df['Bucket'] == 2]['Name'].count()\n",
    "    small_bucket_count = df[df['Bucket'] == 1]['Name'].count()\n",
    "    extra_bucket_count = df[df['Bucket'] == 3]['Name'].count()\n",
    "\n",
    "    # If either bucket is less than a third of the total nubmer of samples, I need to throw an exception.\n",
    "    cutoff_length = int(len(df['Bucket'])/3)\n",
    "\n",
    "    # The threshold was too large.\n",
    "    if large_bucket_count < cutoff_length or small_bucket_count < cutoff_length:\n",
    "        logger.error('Threshold of %3.3f was too large. Large Bucket Size: %i, Small Bucket Size: %i' \n",
    "                     %(threshold, large_bucket_count, small_bucket_count))\n",
    "\n",
    "    # The threshold isn't too large.\n",
    "    else:\n",
    "        logger.info('Threshold of %3.3f provides Large bucket size: %i, Small Bucket size: %i'\n",
    "                    %(threshold, large_bucket_count, small_bucket_count))\n",
    "        x = df[df.columns[1:573]]\n",
    "        y = df[df.columns[575]]\n",
    "\n",
    "        # Create the feature set for the 3 classifiers.\n",
    "        rbf_params = {'gamma': [1e-2, 1e-3, 1e-4], 'C': [1, 10, 100, 1000]}\n",
    "        xgb_params = {'n_estimators': [50, 100, 150], 'max_depth': [1, 2, 3, 4]}\n",
    "        rf_params = {'class_weight': ['balanced'], 'n_estimators': [50, 100, 150], 'max_depth': [1, 2, 3, 4], \n",
    "                       'min_samples_leaf': [1, 2, 3], 'min_samples_split': [1, 2, 3, 4]}\n",
    "\n",
    "        # Classifier pipeline for all 3 classifiers.\n",
    "        logger.info('\\nSVC w/RBF Kernel Results:')\n",
    "        classifier_pipeline(x, y, SVC(kernel='rbf'), rbf_params)\n",
    "        logger.info('\\nXGBoost Classifier Results:')\n",
    "        classifier_pipeline(x, y, XGBClassifier(), xgb_params)\n",
    "        logger.info('\\nRandom Forest Classifier Results:')\n",
    "        classifier_pipeline(x, y, RandomForestClassifier(), rf_params)\n",
    "\n",
    "    # Formatting for the logger.\n",
    "    logger.info('-----------------------------------------------------')\n",
    "    logger.info('')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-09 15:46:39,809 | DEBUG | The full dataset has 73 examples.\n",
      "2022-08-09 15:46:39,812 | INFO | Threshold of 5.000 provides Large bucket size: 25, Small Bucket size: 39\n",
      "2022-08-09 15:46:39,813 | INFO | \n",
      "SVC w/RBF Kernel Results:\n",
      "2022-08-09 15:46:39,813 | DEBUG | Forward Selection Starting\n",
      "2022-08-09 15:58:05,606 | DEBUG | Forward Selection Finished\n",
      "2022-08-09 15:58:05,606 | INFO | -----------------------------------------------------\n",
      "2022-08-09 15:58:05,607 | INFO | \n"
     ]
    }
   ],
   "source": [
    "threshold = 5\n",
    "df, _ = import_data()\n",
    "\n",
    "# Creates a column in our dataframe to classify into 3 separate buckets.  A 'small' and 'large' bucket\n",
    "# based on the threshold, and a 'do not measure bucket' for anything with a KI value of > 4000\n",
    "df['Bucket'] = pd.cut(x=df['KI (nM)'], bins=(0, threshold, 4000, float('inf')), labels=(1,2,3))\n",
    "\n",
    "# Try basing the threshold off of log transform?\n",
    "\n",
    "large_bucket_count = df[df['Bucket'] == 2]['Name'].count()\n",
    "small_bucket_count = df[df['Bucket'] == 1]['Name'].count()\n",
    "extra_bucket_count = df[df['Bucket'] == 3]['Name'].count()\n",
    "\n",
    "# If either bucket is less than a third of the total nubmer of samples, I need to throw an exception.\n",
    "cutoff_length = int(len(df['Bucket'])/3)\n",
    "\n",
    "# The threshold was too large.\n",
    "if large_bucket_count < cutoff_length or small_bucket_count < cutoff_length:\n",
    "    logger.error('Threshold of %3.3f was too large. Large Bucket Size: %i, Small Bucket Size: %i' \n",
    "                    %(threshold, large_bucket_count, small_bucket_count))\n",
    "\n",
    "# The threshold isn't too large.\n",
    "else:\n",
    "    logger.info('Threshold of %3.3f provides Large bucket size: %i, Small Bucket size: %i'\n",
    "                %(threshold, large_bucket_count, small_bucket_count))\n",
    "    x = df[df.columns[1:573]]\n",
    "    y = df[df.columns[575]]\n",
    "\n",
    "    # Create the feature set for the 3 classifiers.\n",
    "    rbf_params = {'gamma': [1e-2, 1e-3, 1e-4], 'C': [1, 10, 100, 1000]}\n",
    "    xgb_params = {'n_estimators': [50, 100, 150], 'max_depth': [1, 2, 3, 4]}\n",
    "    rf_params = {'class_weight': ['balanced'], 'n_estimators': [50, 100, 150], 'max_depth': [1, 2, 3, 4], \n",
    "                    'min_samples_leaf': [1, 2, 3], 'min_samples_split': [1, 2, 3, 4]}\n",
    "\n",
    "    # Classifier pipeline for all 3 classifiers.\n",
    "    logger.info('\\nSVC w/RBF Kernel Results:')\n",
    "    x, _ = fs_classifier(x, y, SVC(kernel='rbf'))\n",
    "    x_pca, _ = principal_component_analysis(x)\n",
    "    #classifIer_trainer(x, y, rbf_params, SVC(kernel='rbf'))    \n",
    "    #logger.info('\\nXGBoost Classifier Results:')\n",
    "    #classifier_pipeline(x, y, XGBClassifier(), xgb_params)\n",
    "    #logger.info('\\nRandom Forest Classifier Results:')\n",
    "    #classifier_pipeline(x, y, RandomForestClassifier(), rf_params)\n",
    "\n",
    "# Formatting for the logger.\n",
    "logger.info('-----------------------------------------------------')\n",
    "logger.info('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-09 16:20:55,826 | DEBUG | GridSearchCV Starting\n",
      "2022-08-09 16:20:55,874 | DEBUG | Best parameters set found on development set:\n",
      "2022-08-09 16:20:55,875 | DEBUG | \n",
      "2022-08-09 16:20:55,875 | DEBUG | {'C': 10, 'gamma': 0.001}\n",
      "2022-08-09 16:20:55,875 | DEBUG | \n",
      "2022-08-09 16:20:55,875 | DEBUG | Grid scores on development set:\n",
      "2022-08-09 16:20:55,876 | DEBUG | \n",
      "2022-08-09 16:20:55,876 | DEBUG | 0.398 (+/-0.169) for {'C': 1, 'gamma': 0.01}\n",
      "2022-08-09 16:20:55,876 | DEBUG | 0.000 (+/-0.000) for {'C': 1, 'gamma': 0.001}\n",
      "2022-08-09 16:20:55,876 | DEBUG | 0.000 (+/-0.000) for {'C': 1, 'gamma': 0.0001}\n",
      "2022-08-09 16:20:55,876 | DEBUG | 0.430 (+/-0.345) for {'C': 10, 'gamma': 0.01}\n",
      "2022-08-09 16:20:55,877 | DEBUG | 0.442 (+/-0.281) for {'C': 10, 'gamma': 0.001}\n",
      "2022-08-09 16:20:55,877 | DEBUG | 0.000 (+/-0.000) for {'C': 10, 'gamma': 0.0001}\n",
      "2022-08-09 16:20:55,877 | DEBUG | 0.369 (+/-0.351) for {'C': 100, 'gamma': 0.01}\n",
      "2022-08-09 16:20:55,877 | DEBUG | 0.354 (+/-0.475) for {'C': 100, 'gamma': 0.001}\n",
      "2022-08-09 16:20:55,878 | DEBUG | 0.353 (+/-0.160) for {'C': 100, 'gamma': 0.0001}\n",
      "2022-08-09 16:20:55,878 | DEBUG | 0.355 (+/-0.310) for {'C': 1000, 'gamma': 0.01}\n",
      "2022-08-09 16:20:55,878 | DEBUG | 0.404 (+/-0.237) for {'C': 1000, 'gamma': 0.001}\n",
      "2022-08-09 16:20:55,878 | DEBUG | 0.330 (+/-0.468) for {'C': 1000, 'gamma': 0.0001}\n",
      "2022-08-09 16:20:55,878 | DEBUG | \n",
      "2022-08-09 16:20:55,879 | DEBUG | Training:\n",
      "2022-08-09 16:20:55,881 | DEBUG | Training Finished.\n",
      "2022-08-09 16:20:55,884 | INFO | Training Accuracy: 0.897, Training MCC: 0.834, Validation Accuracy: 0.733, Validation MCC: 0.442, Fold: 1\n",
      "2022-08-09 16:20:55,885 | DEBUG | Training:\n",
      "2022-08-09 16:20:55,886 | DEBUG | Training Finished.\n",
      "2022-08-09 16:20:55,888 | INFO | Training Accuracy: 0.879, Training MCC: 0.802, Validation Accuracy: 0.733, Validation MCC: 0.360, Fold: 2\n",
      "2022-08-09 16:20:55,889 | DEBUG | Training:\n",
      "2022-08-09 16:20:55,890 | DEBUG | Training Finished.\n",
      "2022-08-09 16:20:55,892 | INFO | Training Accuracy: 0.931, Training MCC: 0.888, Validation Accuracy: 0.867, Validation MCC: 0.686, Fold: 3\n",
      "2022-08-09 16:20:55,892 | DEBUG | Training:\n",
      "2022-08-09 16:20:55,894 | DEBUG | Training Finished.\n",
      "2022-08-09 16:20:55,897 | INFO | Training Accuracy: 0.879, Training MCC: 0.787, Validation Accuracy: 0.533, Validation MCC: 0.248, Fold: 4\n",
      "2022-08-09 16:20:55,897 | DEBUG | Training:\n",
      "2022-08-09 16:20:55,899 | DEBUG | Training Finished.\n",
      "2022-08-09 16:20:55,901 | INFO | Training Accuracy: 0.897, Training MCC: 0.823, Validation Accuracy: 0.667, Validation MCC: 0.390, Fold: 5\n",
      "2022-08-09 16:20:55,901 | INFO | AVG Training Accuracy: 0.897, AVG Training MCC: 0.827, AVG Validation Accuracy: 0.707, AVG Validation MCC: 0.425\n"
     ]
    }
   ],
   "source": [
    "classifier_trainer(x, y, rbf_params, SVC(kernel='rbf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, gamma=0.001)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(kernel='rbf')\n",
    "model.set_params(**optimized_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_clf = load('bucket_clf.joblib')\n",
    "bucket_sfs = load('bucket_sfs.joblib')\n",
    "bucket_pca = load('bucket_pca.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[df.columns[1:573]]\n",
    "x = bucket_sfs.transform(x)\n",
    "x = bucket_pca.transform(x)\n",
    "df['Bucket'] = bucket_clf.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large = df[df['Bucket'] == True]\n",
    "df_small = df[df['Bucket'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_large = df_large[df_large.columns[1:573]]\n",
    "y_log_large = df_large[df_large.columns[574]]\n",
    "y_large = df_large[df_large.columns[573]]\n",
    "\n",
    "x_small = df_small[df_small.columns[1:573]]\n",
    "y_log_small = df_small[df_small.columns[574]]\n",
    "y_small = df_small[df_small.columns[573]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward Selection on the datasets.  Then apply principal component analysis.\n",
    "\n",
    "\n",
    "## Perform forward selection using svr w/RBF kernel\n",
    "def fs_regressor(x, y):\n",
    "    \"\"\"\n",
    "    Perform Sequentual Forward Selection on a given dataset.  This will\n",
    "        return half of the features of the initial dataset.  The model used\n",
    "        is SVR w/RBF kernel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: Input values of the dataset.\n",
    "\n",
    "    y: Output values of the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: Input values of the dataset with half the features selected.\n",
    "\n",
    "    \"\"\"\n",
    "    # Fit a feature selector to SVR w/RBF kernel regressor and use the MSE score.\n",
    "\n",
    "    logger.debug('Forward Selection Starting')\n",
    "    reg = SVR(kernel='rbf')\n",
    "    sfs = SequentialFeatureSelector(reg, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "    sfs.fit(x, y)\n",
    "    x = sfs.transform(x)\n",
    "    logger.debug('Forward Selection Finished')\n",
    "\n",
    "    return x, sfs\n",
    "\n",
    "# Apply Forward Selectio non the large and small buckets.\n",
    "x_large, _ = fs_regressor(x_large, y_log_large)\n",
    "x_small, _ = fs_regressor(x_small, y_log_small)\n",
    "\n",
    "# Apply PCA to the selected features\n",
    "x_large, _ = principal_component_analysis(x_large)\n",
    "x_small, _ = principal_component_analysis(x_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_large_train, x_large_valid, y_log_large_train, y_log_large_valid = train_test_split(x_large, y_log_large, test_size=0.2, random_state=42)\n",
    "_, _, y_large_train, y_large_valid = train_test_split(x_large, y_large, test_size=0.2, random_state=42)\n",
    "\n",
    "x_small_train, x_small_valid, y_log_small_train, y_log_small_valid = train_test_split(x_small, y_log_small, test_size=0.2, random_state=42)\n",
    "_, _, y_small_train, y_small_valid = train_test_split(x_small, y_small, test_size=0.2, random_state=42)\n",
    "\n",
    "reg_large = SVR(kernel='rbf')\n",
    "reg_large.fit(x_large_train, y_log_large_train)\n",
    "y_log_large_pred = reg_large.predict(x_large_valid)\n",
    "rmse_log_large = np.sqrt(mean_squared_error(y_log_large_valid, y_log_large_pred))\n",
    "y_large_pred = unscale(y_log_large_pred, base_range)\n",
    "rmse_large = np.sqrt(mean_squared_error(y_large_valid, y_large_pred))\n",
    "\n",
    "\n",
    "reg_small = SVR(kernel='rbf')\n",
    "reg_small.fit(x_small_train, y_log_small_train)\n",
    "y_log_small_pred = reg_small.predict(x_small_valid)\n",
    "rmse_log_small = np.sqrt(mean_squared_error(y_log_small_valid, y_log_small_pred))\n",
    "y_small_pred = unscale(y_log_small_pred, base_range)\n",
    "rmse_small = np.sqrt(mean_squared_error(y_small_valid, y_small_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['KI (nM) unscaled'] = unscale(df['KI (nM) rescaled'], base_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[df.columns[1:573]]\n",
    "y_log = df[df.columns[574]]\n",
    "y = df[df.columns[573]]\n",
    "\n",
    "x, _ = fs_regressor(x, y_log)\n",
    "x, _ = principal_component_analysis(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_log_train, y_log_valid = train_test_split(x, y_log, test_size=0.2, random_state=42)\n",
    "_, _, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "reg = SVR(kernel='rbf')\n",
    "reg.fit(x_train, y_log_train)\n",
    "y_log_pred = reg.predict(x_valid)\n",
    "rmse_log = np.sqrt(mean_squared_error(y_log_valid, y_log_pred))\n",
    "y_pred = unscale(y_log_pred, base_range)\n",
    "rmse = np.sqrt(mean_squared_error(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Log of RMSE for the large bucket: %2.2f' %(rmse_log_large))\n",
    "print('RMSE for the large bucket: %2.2f' %(rmse_large))\n",
    "print('')\n",
    "\n",
    "print('Log of RMSE for the small bucket: %2.2f' %(rmse_log_small))\n",
    "print('RMSE for the small bucket: %2.2f' %(rmse_small))\n",
    "print('')\n",
    "\n",
    "print('Log of RMSE for all data: %2.2f' %(rmse_log))\n",
    "print('RMSE for all data: %2.2f' %(rmse))\n",
    "print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('tensorflow_m1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4cc95c124cad13857ab8503a0b05d1c4503d9111f9b4dd2c5d2cc598f0f99dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
