{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Dependencies\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    " \n",
    "# Metrics\n",
    "from sklearn.metrics import matthews_corrcoef, mean_squared_error, accuracy_score, make_scorer\n",
    "\n",
    "# Model Persistence\n",
    "from joblib import dump, load\n",
    "\n",
    "# Plotter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Argument Parser\n",
    "import argparse\n",
    "\n",
    "# Write to a log file\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the complete dataset.\n",
    "def import_data():\n",
    "    \"\"\"\n",
    "    Import the full dataset from the current path.  Also apply some of the necessary preprocessing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df:  Dataframe of the full KI training dataset, with any values above 50,000 removed.\n",
    "\n",
    "    base_range:  Contains the range of values within the dataframe for rescaling purposes.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Importing the full KI set into a dataframe.\n",
    "    path = os.getcwd()\n",
    "    df = pd.read_csv(path + '/PositivePeptide_Ki.csv')\n",
    "\n",
    "    # Rescaling the dataframe in the log10 (-5,5) range.\n",
    "    df['KI (nM) rescaled'], base_range  = rescale(df['KI (nM)'], destination_interval=(-5,5))\n",
    "\n",
    "    return df, base_range\n",
    "\n",
    "## Logarithmically scalling the values.\n",
    "def rescale(array=np.array(0), destination_interval=(-5,5)):\n",
    "    \"\"\"\n",
    "    Rescale the KI values from nM to a log scale within the range of\n",
    "        a given destination interval.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array:  A numpy array of KI values, in nM.\n",
    "\n",
    "    destination_interval: the interval that we set the range of the log scale to\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array:  Transformed array into the log scale.\n",
    "    \n",
    "    saved_range:  The (min, max) range of the original given array.  Used if we need\n",
    "        to rescale back into \"KI (nM)\" form.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Rescaling the values and saving the initial range.\n",
    "    array = np.log(array)\n",
    "    saved_range = (array.min(), array.max())\n",
    "    array = np.interp(array, saved_range, destination_interval)\n",
    "\n",
    "    return array, saved_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_name_model_zipper():\n",
    "    \"\"\"\n",
    "    Zips up and creates \n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the feature set for the 3 classifiers.  Put them all into an array.\n",
    "    rbf_params = {'gamma': [1e-1,1e-2,1e-3,1e-4,'scale','auto'], 'C': [5,10,50,100,250,500,1000],\n",
    "                  'class_weight': [None,'balanced'], 'break_ties': [False,True]}\n",
    "    xgb_params = {'max_depth': np.arange(2,11,1), 'n_estimators': np.arange(1,25,1), 'gamma': np.arange(0,4,1),\n",
    "                  'subsample': [0.5,1], 'lambda': [1,5,9], 'alpha': np.arange(0,1.1,0.2)}\n",
    "    rfc_params = {'criterion': ['gini','entropy'], 'max_features': ['sqrt','log2',1.0,0.3], 'ccp_alpha': np.arange(0,0.3,0.1),\n",
    "                  'n_estimators': np.arange(1,25,1), 'max_depth': np.arange(2,11,1)}\n",
    "    params_list = [rbf_params, xgb_params, rfc_params]\n",
    "\n",
    "    # Create the string titles for the various models.\n",
    "    rbf_name = 'SVC with RBF Kernel'\n",
    "    xgb_name = 'XGBoost Classifier'\n",
    "    rfc_name = 'Random Forest Classifier'\n",
    "    names = [rbf_name, xgb_name, rfc_name]\n",
    "\n",
    "    # Create the models.  We've selected our 'base' hyperparameters from earlier.\n",
    "\n",
    "    # Mean_Test_Score = 0.394548, Std_Test_score = 0.133936, mean_train_score = 0.94489, std_train_score = 0.022332.  It looks like\n",
    "    # when gamma = 0.01, C = 1 and when gamma = 0.001, c = 100.  break_ties can be true or false.  no effect.  class weight always 'None'\n",
    "    rbf = SVC(C=10,gamma=0.01,break_ties=True,class_weight=None)\n",
    "\n",
    "    # mean_test_score = 0.43961, std_test_score = 0.0879, mean_train_score = 0.854369, std_train_score = 0.32907\n",
    "    # Max depth doesn't seem to matter too much past 3.\n",
    "    # Another parameter set can be {'alpha': 0.4, 'gamma': 0, 'lambda': 5, 'max_depth': 9, 'n_estimators': 9, 'subsample': 0.5}\n",
    "    # mean_test_score = 0.466982, std_test_score = 0.070494, mean_train_score = 0.815888, std_train_score = 0.049071\n",
    "    xgb = XGBClassifier(alpha=1.0,gamma=1,reg_lambda=1,max_depth=4,n_estimators=22,subsample=0.5)\n",
    "\n",
    "    # mean_test_score = 0.465951, std_test_score = 0.056089, mean_test_score = 0.894403, std_train_score = 0.01336\n",
    "    rfc = RandomForestClassifier(ccp_alpha=0.0,criterion='gini',max_depth=3,max_features='sqrt',n_estimators=23)\n",
    "    models = [rbf, xgb, rfc]\n",
    "\n",
    "    # In a for loop, create a directory for the 3 models and then deposit the hyperparameter tuning results as well\n",
    "    #   as the SFS and PCA models/\n",
    "    attributes = zip(params_list, names, models)\n",
    "\n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "threshold = 0.01\n",
    "\n",
    "# DataFrame importing and adding 'Bucket' column\n",
    "df, _ = import_data()\n",
    "df['Bucket'] = pd.cut(x=df['KI (nM)'], bins=(0, threshold, 4000, float('inf')), labels=(0,1,2))\n",
    "\n",
    "# Get x and y values.\n",
    "x = df[df.columns[1:573]]\n",
    "y = df['Bucket']\n",
    "\n",
    "# Add minMaxScaler here to reduce overfitting.\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x)\n",
    "x = pd.DataFrame(scaler.transform(x), columns=df.columns[1:573])\n",
    "\n",
    "attributes = param_name_model_zipper()\n",
    "\n",
    "for params, name, model in attributes:\n",
    "    x = df[df.columns[1:573]]\n",
    "    sfs = load(path + '/%s/sfs/%s %2.2f fs.joblib' %(name, name, threshold))\n",
    "    x = sfs.transform(x)\n",
    "\n",
    "    if os.path.exists(path + '/%s/PCA Tuning' %(name)) == False:\n",
    "        os.mkdir('%s/PCA Tuning' %(name))\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('antithrombin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e80bae16222c7bbbaf59498086a658280d426110f2738491d72ed8ab66400de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
