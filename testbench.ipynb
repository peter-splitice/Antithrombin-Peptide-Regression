{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Dependencies\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    " \n",
    "# Metrics\n",
    "from sklearn.metrics import matthews_corrcoef, mean_squared_error, accuracy_score, make_scorer\n",
    "\n",
    "# Model Persistence\n",
    "from joblib import dump, load\n",
    "\n",
    "# Plotter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Argument Parser\n",
    "import argparse\n",
    "\n",
    "# Write to a log file\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the complete dataset.\n",
    "def import_data(threshold):\n",
    "    \"\"\"\n",
    "    Import the full dataset from the current path.  Also apply some of the necessary preprocessing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df:  Dataframe of the full KI training dataset, with any values above 50,000 removed.\n",
    "\n",
    "    base_range:  Contains the range of values within the dataframe for rescaling purposes.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Importing the full KI set into a dataframe.\n",
    "    path = os.getcwd()\n",
    "    df = pd.read_csv(path + '/PositivePeptide_Ki.csv')\n",
    "    \n",
    "\n",
    "    # Rescaling the dataframe in the log10 (-5,5) range.\n",
    "    df['KI (nM) rescaled'], base_range  = rescale(df['KI (nM)'], destination_interval=(-5,5))\n",
    "    df['Bucket'] = pd.cut(x=df['KI (nM)'], bins=(0, threshold, 4000, float('inf')), labels=(0,1,2))\n",
    "    return df, base_range\n",
    "\n",
    "## Logarithmically scalling the values.\n",
    "def rescale(array=np.array(0), destination_interval=(-5,5)):\n",
    "    \"\"\"\n",
    "    Rescale the KI values from nM to a log scale within the range of\n",
    "        a given destination interval.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array:  A numpy array of KI values, in nM.\n",
    "\n",
    "    destination_interval: the interval that we set the range of the log scale to\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array:  Transformed array into the log scale.\n",
    "    \n",
    "    saved_range:  The (min, max) range of the original given array.  Used if we need\n",
    "        to rescale back into \"KI (nM)\" form.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Rescaling the values and saving the initial range.\n",
    "    array = np.log(array)\n",
    "    saved_range = (array.min(), array.max())    \n",
    "    array = np.interp(array, saved_range, destination_interval)\n",
    "\n",
    "    return array, saved_range\n",
    "\n",
    "## Inverse of the rescale function to rescale the outputs.\n",
    "def unscale(array, destination_interval, source_interval=(-5,5)):\n",
    "    \"\"\"\n",
    "    Rescales an array of log-transformed values back into \"KI (nM)\" form.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array:  A numpy array of KI values in log-transformed form.\n",
    "\n",
    "    destination_interval:  The original range of KI values.\n",
    "\n",
    "    source_interval: The current range of KI log transformed values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array:  A numpy array of the KI values back in the original format.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Undoing the previous rescaling.\n",
    "    array = np.interp(array, source_interval, destination_interval)\n",
    "    array = np.exp(array)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_clf():\n",
    "    \"\"\"\n",
    "    This section runs the finalized classification portion of the data across the various model types to bucketize our data\n",
    "        for inference.\n",
    "        \n",
    "        - SVC w/RBF Kernel w/SFS and PCA @80% variance. {'C': 61, 'break_ties': True, 'class_weight': None, 'gamma': 0.001},\n",
    "            Test MCC = 0.529094, Train MCC = 0.713933, Threshold @ 10.  Large Bucket Size 20, Small Bucket Size 44, Extra Bucket Size\n",
    "            9.\n",
    "\n",
    "        - XGBoost Classifier w/SFS. {'alpha': 0.0, 'gamma': 2, 'lambda': 1, 'max_depth': 2, 'n_estimators': 11, 'subsample': 0.5},\n",
    "            Test MCC = 0.661811, Train MCC = 0.709423, Threshold @ 0.01.  Large Bucket Size 46, Small Bucket Size 18, Extra Bucket Size\n",
    "            9.\n",
    "\n",
    "        - Random Forest Classifier w/SFS and PCA @85% variance.  {'ccp_alpha': 0.1, 'criterion': 'gini', 'max_depth': 9, \n",
    "            'max_features': 1.0, 'n_estimators': 7}, Test MCC = 0.614015, Train MCC = 0.729953, Threshold @ 10.  Large Bucket Size 20,\n",
    "            Small Bucket Size 44, Extra Bucket Size 9.\n",
    "\n",
    "        - KNN Classifier w/SFS and PCA @100% variance. {'leaf_size': 5, 'n_neighbors': 7, 'p': 2, 'weights': 'uniform'}, \n",
    "            Test MCC = 0.61151, Train MCC = 0.564734, Threshold @10.  Large Bucket Size 20, Small Bucket Size 44, Extra Bucket Size 9.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Create the models with the relevant hyperparameters.\n",
    "    rbf = SVC(kernel='rbf', C=61, break_ties=True, class_weight=None, gamma=0.001)\n",
    "    rbf_threshold = 10\n",
    "    rbf_var = 80\n",
    "\n",
    "    xgb = XGBClassifier(alpha=0.0, gamma=2, reg_lambda=1, max_depth=2, n_estimators=11, subsample=0.5)\n",
    "    xgb_threshold = 0.01\n",
    "    xgb_var = False         # Variance of 'False' indicates that we will not be \n",
    "\n",
    "    rfc = RandomForestClassifier(ccp_alpha=0.1, criterion='gini', max_depth=9, max_features=1.0, n_estimators=7)\n",
    "    rfc_threshold = 10\n",
    "    rfc_var = 85\n",
    "\n",
    "    knn = KNeighborsClassifier(leaf_size=5, n_neighbors=7, p=2, weights='uniform')\n",
    "    knn_threshold = 10\n",
    "    knn_var = 100\n",
    "\n",
    "    # Put the model information in 4 different list.  Models, Thresholds, Variances, and Names\n",
    "    models = [rbf, xgb, rfc, knn]\n",
    "    thresholds = [rbf_threshold, xgb_threshold, rfc_threshold, knn_threshold]\n",
    "    vars = [rbf_var, xgb_var, rfc_var, knn_var]\n",
    "    names = ['SVC with RBF Kernel', 'XGBoost Classifier', 'Random Forest Classifier', 'KNN Classifier']\n",
    "\n",
    "    saved_clf = zip(models,thresholds,vars, names)\n",
    "    return saved_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10\n",
    "seeds = [33, 42, 55, 68, 74]\n",
    "i = 0\n",
    "\n",
    "folds = len(seeds)\n",
    "\n",
    "df, ki_range = import_data(threshold)\n",
    "path = os.getcwd()\n",
    "\n",
    "x = df[df.columns[1:573]]\n",
    "y = df['KI (nM) rescaled']\n",
    "buckets = df['Bucket']\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=(1/folds), random_state=42, stratify=buckets)\n",
    "\n",
    "# Create a small and large training set\n",
    "buckets_train = buckets[y_train.index]\n",
    "buckets_valid = buckets[y_valid.index]\n",
    "\n",
    "x_train_sml = x_train[buckets_train==0]\n",
    "buckets_train_sml = buckets_train[buckets_train==0]\n",
    "y_train_sml = y_train[buckets_train==0]\n",
    "\n",
    "x_train_med = x_train[buckets_train==1]\n",
    "buckets_train_med = buckets_train[buckets_train==1]\n",
    "y_train_med = y_train[buckets_train==1]\n",
    "\n",
    "reg_sml = SVR()\n",
    "reg_sml.fit(x_train_sml, y_train_sml)\n",
    "reg_med = SVR()\n",
    "reg_med.fit(x_train_med, y_train_med)\n",
    "\n",
    "saved_clf = load_saved_clf()\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "[0 0 0 0 1 0 0 0 1 1 1 0 0 1 0]\n",
      "[0 0 0 0 1 0 1 1 0 0 1 1 0 1 0]\n",
      "[0 0 0 0 1 0 1 0 1 0 2 0 0 0 0]\n",
      "another\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a1351\\.conda\\envs\\antithrombin\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator SequentialFeatureSelector from version 1.1.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bucketflag=0\n",
    "\n",
    "for clf, threshold, var, name in saved_clf:\n",
    "\n",
    "    # Now we need to train and apply our classifier on the original dataset.  I think we should apply whichever transformations\n",
    "    #   already existing on that part of the pipeline.  We should already have the saved .joblib files so this should be easier.\n",
    "\n",
    "    # Apply Sequential Feature Selection to the x_train values.\n",
    "    sfs = load(path + '/%s/sfs/%s %2.2f fs.joblib' %(name, name, threshold))\n",
    "    x_train_clf = sfs.transform(x_train)\n",
    "    x_valid_clf = sfs.transform(x_valid)\n",
    "\n",
    "    # Apply PCA if applicable with the necessary var.\n",
    "    if var != False:\n",
    "        pca = load(path + '/%s/sfs-pca/%s %2.2f pca.joblib' %(name, name, threshold))\n",
    "        x_train_clf = pca.transform(x_train_clf)\n",
    "        x_valid_clf = pca.transform(x_valid_clf)\n",
    "\n",
    "        # Dimensonality Reduction based on accepted variance.\n",
    "        ratios = np.array(pca.explained_variance_ratio_)\n",
    "        ratios = ratios[ratios.cumsum() <= (var/100)]\n",
    "        \n",
    "        # Readjust the dimensions of x based on the variance we want.\n",
    "        length = len(ratios)\n",
    "        x_train_clf = x_train_clf[:,0:length]\n",
    "        x_valid_clf = x_valid_clf[:,0:length]\n",
    "\n",
    "    clf.fit(x_train_clf, buckets_train)\n",
    "\n",
    "    # Apply the transformations to the Validation set:\n",
    "    bucket_valid = clf.predict(x_valid_clf)\n",
    "    print(bucket_valid)\n",
    "\n",
    "print('another')\n",
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_reg = x_valid[bucket_valid==bucketflag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [15, 11]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\a1351\\OneDrive - sjsu.edu\\Antithrombin Feature Extraction\\Regression\\testbench.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/a1351/OneDrive%20-%20sjsu.edu/Antithrombin%20Feature%20Extraction/Regression/testbench.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y_pred \u001b[39m=\u001b[39m reg_sml\u001b[39m.\u001b[39mpredict(x_valid_reg)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/a1351/OneDrive%20-%20sjsu.edu/Antithrombin%20Feature%20Extraction/Regression/testbench.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m alid_accuracy \u001b[39m=\u001b[39m accuracy_score(y_valid, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\a1351\\.conda\\envs\\antithrombin\\lib\\site-packages\\sklearn\\metrics\\_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    213\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\a1351\\.conda\\envs\\antithrombin\\lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m     \u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     85\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\a1351\\.conda\\envs\\antithrombin\\lib\\site-packages\\sklearn\\utils\\validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    385\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    390\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [15, 11]"
     ]
    }
   ],
   "source": [
    "y_pred = reg_sml.predict(x_valid_reg)\n",
    "alid_accuracy = accuracy_score(y_valid, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('antithrombin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "693f678ed88af8512f5e94971fc2af07d2c48cb96c1bd89b41f580d2bfa05b8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
