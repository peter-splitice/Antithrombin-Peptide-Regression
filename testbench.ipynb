{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Dependencies\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    " \n",
    "# Metrics\n",
    "from sklearn.metrics import matthews_corrcoef, mean_squared_error, accuracy_score, make_scorer\n",
    "\n",
    "# Model Persistence\n",
    "from joblib import dump, load\n",
    "\n",
    "# Plotter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Argument Parser\n",
    "import argparse\n",
    "\n",
    "# Write to a log file\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the complete dataset.\n",
    "def import_data(threshold):\n",
    "    # Importing the full KI set into a dataframe.\n",
    "    path = os.getcwd()\n",
    "    df = pd.read_csv(path + '/PositivePeptide_Ki.csv')\n",
    "    \n",
    "\n",
    "    # Rescaling the dataframe in the log10 (-5,5) range.\n",
    "    df['KI (nM) rescaled'], base_range  = rescale(df['KI (nM)'], destination_interval=(-5,5))\n",
    "    df['Bucket'] = pd.cut(x=df['KI (nM)'], bins=(0, threshold, 4000, float('inf')), labels=(0,1,2))\n",
    "    return df, base_range\n",
    "\n",
    "## Logarithmically scalling the values.\n",
    "def rescale(array=np.array(0), destination_interval=(-5,5)):\n",
    "    # Rescaling the values and saving the initial range.\n",
    "    array = np.log(array)\n",
    "    saved_range = (array.min(), array.max())    \n",
    "    array = np.interp(array, saved_range, destination_interval)\n",
    "\n",
    "    return array, saved_range\n",
    "\n",
    "## Inverse of the rescale function to rescale the outputs.\n",
    "def unscale(array, destination_interval, source_interval=(-5,5)):\n",
    "    # Undoing the previous rescaling.\n",
    "    array = np.interp(array, source_interval, destination_interval)\n",
    "    array = np.exp(array)\n",
    "\n",
    "    return array\n",
    "\n",
    "def hyperparameter_optimizer(x, y, params, model=SVR()):\n",
    "    reg = GridSearchCV(model, param_grid=params, scoring='neg_root_mean_squared_error', cv=5, return_train_score=True,\n",
    "                       n_jobs=-1)\n",
    "    reg.fit(x,y)\n",
    "\n",
    "\n",
    "    # Testing on the development set.  Save the results to a pandas dataframe and then sort it by\n",
    "    # standard deviation of the test set.\n",
    "    df = pd.DataFrame(reg.cv_results_)\n",
    "    index = reg.best_index_\n",
    "    scores = [df['mean_train_score'][index], df['std_train_score'][index], reg.best_score_, df['std_test_score'][index], reg.best_params_]\n",
    "\n",
    "    df = df.sort_values(by=['std_test_score'])\n",
    "\n",
    "    # Clean up the output for the hyperparameters.  Eliminate any values that have too low of a test ranking\n",
    "    #   as well as eliminate anything with too high of a training score.\n",
    "    max_test_rank = df['rank_test_score'].max()\n",
    "    col_start = 'split0_train_score'\n",
    "    index_start = df.columns.get_loc(col_start)\n",
    "    df = df[~(df.iloc[:,index_start:]>0.98).any(1)]\n",
    "    df = df[df['mean_train_score'] > 0.65]\n",
    "    df = df[df['rank_test_score'] < (0.20*max_test_rank)]\n",
    "    df = df[df['mean_test_score'] > 0.25]\n",
    "\n",
    "    # Save the best parameters.\n",
    "    bestparams = reg.best_params_\n",
    "\n",
    "    model.set_params(**bestparams)\n",
    "\n",
    "    return model, df, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_clf():\n",
    "    # Create the models with the relevant hyperparameters.\n",
    "    rbf = SVC(kernel='rbf', C=61, break_ties=True, class_weight=None, gamma=0.001)\n",
    "    xgb = XGBClassifier(alpha=0.0, gamma=2, reg_lambda=1, max_depth=2, n_estimators=11, subsample=0.5)\n",
    "    rfc = RandomForestClassifier(ccp_alpha=0.1, criterion='gini', max_depth=9, max_features=1.0, n_estimators=7)\n",
    "    knn = KNeighborsClassifier(leaf_size=5, n_neighbors=7, p=2, weights='uniform')\n",
    "    \n",
    "    models = [rbf, xgb, rfc, knn]\n",
    "    thresholds = [10, 0.01, 10, 10]\n",
    "    variances = [80, False, 85, 100]\n",
    "    names = ['SVC with RBF Kernel', 'XGBoost Classifier', 'Random Forest Classifier', 'KNN Classifier']\n",
    "\n",
    "    saved_clf = list(zip(thresholds, variances, names, models))\n",
    "\n",
    "    return saved_clf\n",
    "\n",
    "def load_regression_models():\n",
    "    rbf_params = {}\n",
    "    lin_params = {}\n",
    "    las_params = {}\n",
    "    \n",
    "    # names and hyperparameters to sort through are shared.\n",
    "    names = ['SVR with RBF Kernel', 'SVR with Linear Kernel', 'Lasso Regression']\n",
    "    params = [rbf_params, lin_params, las_params]\n",
    "\n",
    "    # I need to instantiate new models for both the small and medium buckets.\n",
    "    sml_models = [SVR(kernel='rbf'), SVR(kernel='linear'), Lasso()]\n",
    "    med_models = [SVR(kernel='rbf'), SVR(kernel='linear'), Lasso()]\n",
    "\n",
    "    # Create the lists.\n",
    "    reg_models = list(zip(names, params, sml_models, med_models))\n",
    "\n",
    "    return reg_models\n",
    "\n",
    "saved_clf = load_saved_clf()\n",
    "threshold, var, name, clf = saved_clf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x, y, buckets, ki_range, clf=SVC(), sml_reg=SVR(), med_reg=SVR()):\n",
    "\n",
    "    # Training set\n",
    "    # Bucketize\n",
    "    buckets_actual = buckets[y.index]\n",
    "    buckets_pred = clf.predict(x)\n",
    "\n",
    "    # Make predictions for all of the buckets.  The large bucket we'll just predict as 0 for now.\n",
    "    sml_pred = sml_reg.predict(x[buckets_pred==0])\n",
    "    med_pred = med_reg.predict(x[buckets_pred==1])\n",
    "    lrg_pred = np.zeros(np.count_nonzero(x[buckets_pred==2]))\n",
    "\n",
    "    # Put back the predictions in the original order.\n",
    "    y_pred = np.array([])\n",
    "    for i in buckets_pred:\n",
    "        if i == 0:\n",
    "            y_pred = np.append(y_pred, sml_pred[0])\n",
    "            sml_pred = np.delete(sml_pred, 0)\n",
    "        elif i == 1:\n",
    "            y_pred = np.append(y_pred, med_pred[0])\n",
    "            med_pred = np.delete(med_pred, 0)\n",
    "        elif i == 2:\n",
    "            y_pred = np.append(y_pred, lrg_pred[0])\n",
    "            lrg_pred = np.delete(lrg_pred, 0)\n",
    "\n",
    "    y_pred_unscaled = unscale(y_pred, ki_range)\n",
    "    y_unscaled = unscale(y, ki_range)\n",
    "\n",
    "    # RMSE\n",
    "    train_rmse = mean_squared_error(y_unscaled, y_pred_unscaled)**0.5\n",
    "    log_train_rmse = mean_squared_error(y, y_pred)**0.5\n",
    "\n",
    "    # Save the results in a dataframe.\n",
    "    cols = ['Log Y Actual', 'Log Y Predicted', 'Y Actual', 'Y Predicted', 'Actual Bucket', 'Predicted Bucket']\n",
    "    df_data = zip(y, y_pred, y_unscaled, y_pred_unscaled, buckets_actual, buckets_pred)\n",
    "    df = pd.DataFrame(data=df_data, columns=cols)\n",
    "\n",
    "    return train_rmse, log_train_rmse, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and path import.\n",
    "df, ki_range = import_data(threshold)\n",
    "path = os.getcwd()\n",
    "\n",
    "# Extract the x, y, and bucket information.\n",
    "x = df[df.columns[1:573]]\n",
    "y = df['KI (nM) rescaled']\n",
    "buckets = df['Bucket']\n",
    "\n",
    "# Apply MinMaxScaler to the initial x values.\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x)\n",
    "x = pd.DataFrame(scaler.transform(x), columns=df.columns[1:573])\n",
    "\n",
    "# Sequential Feature Selection with the saved model.\n",
    "sfs = load(path + '/%s/sfs/%s %2.2f fs.joblib' %(name, name, threshold))\n",
    "x = sfs.transform(x)\n",
    "\n",
    "# Where applicable, apply PCA tuning as well.\n",
    "if var != False:\n",
    "    pca = load(path + '/%s/sfs-pca/%s %2.2f pca.joblib' %(name, name, threshold))\n",
    "    x = pca.transform(x)\n",
    "\n",
    "    # Dimensonality Reduction based on accepted variance.\n",
    "    ratios = np.array(pca.explained_variance_ratio_)\n",
    "    ratios = ratios[ratios.cumsum() <= (var/100)]\n",
    "    \n",
    "    # Readjust the dimensions of x based on the variance we want.\n",
    "    length = len(ratios)\n",
    "    x = x[:,0:length]\n",
    "\n",
    "# Load up the regression models here:\n",
    "reg_models = load_regression_models()\n",
    "reg_name, reg_params, sml_reg, med_reg = reg_models[0]\n",
    "\n",
    "## Seed values and k-folding required variables.\n",
    "seeds = [33, 42, 55, 68, 74]\n",
    "i = 0\n",
    "folds = len(seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=(1/folds), random_state=42, stratify=buckets)\n",
    "buckets_train = buckets[y_train.index]\n",
    "buckets_valid = buckets[y_valid.index]\n",
    "\n",
    "# Training sets for the each bucket.\n",
    "x_train_sml = x_train[buckets_train==0]\n",
    "y_train_sml = y_train[buckets_train==0]\n",
    "x_train_med = x_train[buckets_train==1]\n",
    "y_train_med = y_train[buckets_train==1]\n",
    "\n",
    "# Fitting the classification and regression models.\n",
    "clf.fit(x_train, buckets_train)\n",
    "sml_reg.fit(x_train_sml, y_train_sml)\n",
    "med_reg.fit(x_train_med, y_train_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse, train_rmse_log, train_df = inference(x_train, y_train, buckets, ki_range, clf, sml_reg, med_reg)\n",
    "valid_rmse, valid_rmse_log, valid_df = inference(x_valid, y_valid, buckets, ki_range, clf, sml_reg, med_reg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('antithrombin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e80bae16222c7bbbaf59498086a658280d426110f2738491d72ed8ab66400de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
