{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Dependencies\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    " \n",
    "# Metrics\n",
    "from sklearn.metrics import matthews_corrcoef, mean_squared_error, accuracy_score, make_scorer\n",
    "\n",
    "# Model Persistence\n",
    "from joblib import dump, load\n",
    "\n",
    "# Plotter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Argument Parser\n",
    "import argparse\n",
    "\n",
    "# Write to a log file\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the complete dataset.\n",
    "def import_data():\n",
    "    \"\"\"\n",
    "    Import the full dataset from the current path.  Also apply some of the necessary preprocessing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df:  Dataframe of the full KI training dataset, with any values above 50,000 removed.\n",
    "\n",
    "    base_range:  Contains the range of values within the dataframe for rescaling purposes.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Importing the full KI set into a dataframe.\n",
    "    path = os.getcwd()\n",
    "    df = pd.read_csv(path + '/PositivePeptide_Ki.csv')\n",
    "    \n",
    "    # Remove all but the 73 features that were relavant in Nivedha's Classification pipeline\n",
    "    extracted_features = pd.read_json('features.json', typ='Series')\n",
    "\n",
    "    # Rescaling the dataframe in the log10 (-5,5) range.\n",
    "    df['KI (nM) rescaled'], base_range  = rescale(df['KI (nM)'], destination_interval=(-5,5))\n",
    "\n",
    "    return df, extracted_features, base_range\n",
    "\n",
    "## Logarithmically scalling the values.\n",
    "def rescale(array=np.array(0), destination_interval=(-5,5)):\n",
    "    \"\"\"\n",
    "    Rescale the KI values from nM to a log scale within the range of\n",
    "        a given destination interval.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array:  A numpy array of KI values, in nM.\n",
    "\n",
    "    destination_interval: the interval that we set the range of the log scale to\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array:  Transformed array into the log scale.\n",
    "    \n",
    "    saved_range:  The (min, max) range of the original given array.  Used if we need\n",
    "        to rescale back into \"KI (nM)\" form.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Rescaling the values and saving the initial range.\n",
    "    array = np.log(array)\n",
    "    saved_range = (array.min(), array.max())\n",
    "    array = np.interp(array, saved_range, destination_interval)\n",
    "\n",
    "    return array, saved_range\n",
    "\n",
    "## Inverse of the rescale function to rescale the outputs.\n",
    "def unscale(array, destination_interval, source_interval=(-5,5)):\n",
    "    \"\"\"\n",
    "    Rescales an array of log-transformed values back into \"KI (nM)\" form.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array:  A numpy array of KI values in log-transformed form.\n",
    "\n",
    "    destination_interval:  The original range of KI values.\n",
    "\n",
    "    source_interval: The current range of KI log transformed values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array:  A numpy array of the KI values back in the original format.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Undoing the previous rescaling.\n",
    "    array = np.interp(array, source_interval, destination_interval)\n",
    "    array = np.exp(array)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10\n",
    "\n",
    "df, extracted_features, _ = import_data()\n",
    "path = os.getcwd()\n",
    "\n",
    "df['Bucket'] = pd.cut(x=df['KI (nM)'], bins=(0, threshold, 4000, float('inf')), labels=(0,1,2))\n",
    "\n",
    "x = df[extracted_features]\n",
    "y = df['Bucket']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x)\n",
    "x = pd.DataFrame(scaler.transform(x), columns=extracted_features)\n",
    "\n",
    "sfs = load(path + '/SVC with RBF Kernel/sfs/SVC with RBF Kernel 10.00 fs.joblib')\n",
    "\n",
    "features_taken = sfs.get_feature_names_out()\n",
    "x = sfs.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_all = PCA()\n",
    "pca_all.fit(x)\n",
    "\n",
    "x_all = pca_all.transform(x)\n",
    "\n",
    "components_all = pca_all.components_\n",
    "variances_all = pca_all.explained_variance_\n",
    "ratios_all = np.array(pca_all.explained_variance_ratio_)\n",
    "\n",
    "np.sum(ratios_all)\n",
    "ratios_max = np.array(ratios_all)\n",
    "ratios_max = ratios_max[ratios_max.cumsum() <= 0.8]\n",
    "length = len(ratios_max)\n",
    "x_reduced = x_all[:,0:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5523198488577367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_5 = PCA(n_components=5)\n",
    "pca_5.fit(x)\n",
    "\n",
    "x_5 = pca_5.transform(x)\n",
    "\n",
    "components_5 = pca_5.components_\n",
    "variances_5 = pca_5.explained_variance_\n",
    "ratios_5 = pca_5.explained_variance_ratio_\n",
    "\n",
    "np.sum(ratios_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent 100%\n"
     ]
    }
   ],
   "source": [
    "percent = 100\n",
    "print('percent %i%%' %(percent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('antithrombin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e80bae16222c7bbbaf59498086a658280d426110f2738491d72ed8ab66400de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
